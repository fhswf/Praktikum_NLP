{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ac460f1",
   "metadata": {},
   "source": [
    "<a href=\"https://www.ki.fh-swf.de/jupyterhub/hub/user-redirect/git-pull?profile=nlp-environment&repo=https%3A%2F%2Fgithub.com%2Ffhswf%2FPraktikum_NLP.git&urlpath=lab%2Ftree%2FPraktikum_NLP.git%2FVeranstaltung_6%2Findex.ipynb&branch=main&profile=nlp-environment\" style=\"\"><img src=\"https://www.ki.fh-swf.de/cluster_badge.svg\" style=\"height: 32px\" alt=\"Open in FH Cluster\"></a>\n",
    "\n",
    "# Textzusammenfassung mit *Transformern*\n",
    "\n",
    "Seit ChatGPT sind von LLMs regelrecht „verwöhnt“. Was können einfachere Modelle im Vergleich dazu leisten, speziell für die deutsche Sprache?\n",
    "\n",
    "## Aufgabe 1: Zusammenfassung deutscher Texte mit vortrainierten Transformermodellen\n",
    "\n",
    "Im Verzeichnis `texte` gibt es einige (deutschsprachige) Textdateien. \n",
    "\n",
    "1. Auswahl geeigneter Modelle\n",
    "   \n",
    "   Auf dem [Model Hub](https://huggingface.co/models) gibt es eine ganze Reihe von Modellen für `Summarization`. Suchen Sie sich einige geeignete Kandidaten für **deutsche** Texte.\n",
    "\n",
    "2. Pipeline für die Zusammenfassung\n",
    "\n",
    "    Mithilfe der Huggingface-Bibliotheken ist das Erstellen einer `pipeline` für die Zusammenfassung ganz einfach:  \n",
    "\n",
    "    ```Python\n",
    "    from transformers import pipeline\n",
    "\n",
    "    summarizer = pipeline(\"summarization\", model=model_name)\n",
    "    summary = summarizer(text, max_length=250, min_length=25, do_sample=True)\n",
    "    \n",
    "    ```\n",
    "\n",
    "3. Zusammenfassung der Texte & Bewertung der Ergebnisse\n",
    "   Erstellen Sie mithilfe der Modelle Zusammenfassingen der Texte. Bewerten Sie in Ihren Gruppen die Ergebnisse.\n",
    "   Welche Einschränkungen stellen Sie fest?\n",
    "   \n",
    "## Aufgabe 2: Vergleich mit ChatGPT\n",
    "\n",
    "(Chat-)GPT ist zwar nicht speziell für die Zusammenfassung von Texten trainiert, kann das aber auch ganz gut.\n",
    "\n",
    "![](Summary_GPT4.png)\n",
    "\n",
    "Testen Sie die Zusammenfassung mittels ChatGPT und vergleichen Sie die Ergebnisse mit denen aus Aufgabe 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06089111-22de-44e7-8da9-44377e4c1807",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Einmalumdiewelt/T5-Base_GNAD\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2416f6b3-270d-4001-a1b7-8da6f04072c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"texte/fhswf.txt\")\n",
    "text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f3162b3-ee89-463a-a0b2-7f80ee4d703a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (537 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "summary = summarizer(text, max_length=250, min_length=25, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d36d241b-a61b-457e-a2df-855edf1a157d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Die Fachhochschule Südwestfalen entstand im Jahr 2002, als die Gesamthochschule in Nordrhein-Westfalen aufgelöst wurde. Der älteste Vorläufer der Hochschule war die Gewerbeschule in Hagen.'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7556426-eba9-43e0-8c6c-c29c24a521d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53693cb6-66ea-4ce7-a41d-31ef989331da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but your input_length is only 198. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=99)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Die Bundesregierung hat laut Ifo-Forschern im laufenden Jahr 64 Milliarden Euro für Verteidigung ausgegeben. Das entspricht 1,6 Prozent der deutschen Wirtschaftsleistung – 17 Milliarden Euro wären nötig.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"texte/verteidigung.txt\")\n",
    "text = file.read()\n",
    "summarizer(text, max_length=250, min_length=25, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "858bc006-c9d2-47b1-a685-149137a5d4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/opt/conda/lib/python3.11/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"ml6team/mt5-small-german-finetune-mlsum\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eee8117-ea4f-4b69-84e4-c1eea6a3a013",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but your input_length is only 200. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Weil die Bundeswehr in Berlin in den kommenden Jahr ins Jahr 65 Milliarden bringen, können neue Länder zahlen.'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"texte/verteidigung.txt\")\n",
    "text = file.read()\n",
    "summarizer(text, max_length=250, min_length=25, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04b1edcf-8e5a-416e-b8ba-bfaccde5d12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"T-Systems-onsite/mt5-small-sum-de-en-v2\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f068391f-640c-4d46-aa86-836ce6cf655a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but your input_length is only 200. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Die Bundeswehr darf im laufenden Jahr 64 Milliarden pro Jahr für Verteidigung ausbußen. Dabei liegt Deutschland immer noch tief unter dem vereinbarten Zwei-Prozent-Ziel der Nato. Das entspräche 1,6 Prozent der deutschen Wirtschaftsleistung, für weitere 16 Milliarden Euro könnten beide Staaten das Ziel nicht erreichen.'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"texte/verteidigung.txt\")\n",
    "text = file.read()\n",
    "summarizer(text, max_length=250, min_length=25, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "906f6e46-24f5-45d5-a8f2-dc35e01fc692",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Shahm/bart-german\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "018998d5-e7d5-4e17-adff-988376611821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Trotz Milliardenausgaben für die Bundeswehr liegt Deutschland immer noch weit unter dem 2014 vereinbarten Zwei-Prozent-Ziel der Nato.'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"texte/verteidigung.txt\")\n",
    "text = file.read()\n",
    "summarizer(text, max_length=250, min_length=25, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9c6fe828-e389-468c-a988-f544fdd053ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31dd73a978fb42ab9d8f2728cdbf98ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/702 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e9b457c8a74776affce64fe1dc9ea8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bddc5ecc4c90424a9a9a17024f002b12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/408 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2babc4931454fb3b4f18de62f8f7cbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/4.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6bf09bdb31446eea3f22505c3301cf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/8.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb95a5926cb40788552e13115f3d90e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/65.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model_name = \"aiautomationlab/german-news-title-gen-mt5\"\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78af02fb-084c-416f-9c1f-568c0324cee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 250, but your input_length is only 200. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'summary_text': 'Deutschland liegt immer noch weit unter zwei-Prozent-Ziel der Nato vertragskonform. Nicht mehr reichen'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"texte/verteidigung.txt\")\n",
    "text = file.read()\n",
    "summarizer(text, max_length=250, min_length=25, do_sample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bc6471-51c9-4d1c-9268-c85442f1ab4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
