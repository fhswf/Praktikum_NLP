{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21dc3826-60ec-46e9-b914-0d4cfc14a7aa",
   "metadata": {},
   "source": [
    "# Textklassifikation, Teil 2\n",
    "\n",
    "Im ersten Teil haben wir Texte aus dem Datensatz *GermEval 2018* mithilfe von Frequency-Vektoren und einem Naive-Bayes-Klassifikator klassifiziert.\n",
    "\n",
    "Dieses Mal verwenden wir ein vortrainiertes *Word2Vec* Modell und wollen untersuchen, ob wir damit bessere Ergebnisse erzielen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6068a489-3553-4e3a-a262-644fbabe3308",
   "metadata": {},
   "source": [
    "## Daten einlesen\n",
    "\n",
    "Wie beim letzten Mal lesen wir zunächst die Daten ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2f351-30b1-453f-8a1e-61bc5911a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_ds = pd.read_csv('../data/GermEval-2018/germeval2018.training.txt', sep='\\t', header=None, names=['text', 'coarse', 'fine'])\n",
    "test_ds = pd.read_csv('../data/GermEval-2018/germeval2018.test.txt', sep='\\t', header=None, names=['text', 'coarse', 'fine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db8cf6b-7154-4ab3-b558-96fb5f957cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd3b27d-4cec-4308-8943-1c1ebde8d802",
   "metadata": {},
   "source": [
    "## Texte bereinigen\n",
    "\n",
    "Auch die Bereinigung der Texte hatten wir schon beim letzten Mal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6fb4753-5947-4983-9d07-6c5643d4b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(text):\n",
    "    \"\"\" Preprocess and tokenize a tweet. \"\"\"\n",
    "    \n",
    "    # remove handles, i.e. @username\n",
    "    text = re.sub('\\@\\w+', '', text)\n",
    "\n",
    "    # remove hashtags, quotes, etc.\n",
    "    text = re.sub('[\\#\"\\']+', '', text)\n",
    "    \n",
    "    # replace hyphens with blanks\n",
    "    text = text.replace('-', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06ef57e-8191-402c-864b-d44ca111b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds['text'] = train_ds['text'].map(clean_tweet)\n",
    "test_ds['text'] = test_ds['text'].map(clean_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca42c9c-bbba-4e92-9226-beac24a7239b",
   "metadata": {},
   "source": [
    "## Word2Vec-Modell laden\n",
    "\n",
    "Die Datei `twitter-de_d100_w5_min10.bin` enthält ein mit deuschen Twitternachrichten trainiertes *Word2Vec* Modell, das wir nun laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5810e049-4724-4abe-a5fb-38533b23d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('/home/archive/nlp/twitter-de_d100_w5_min10.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ca5408c4-fbd4-4b3d-b611-429e5d28e431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.6704499e-01, -2.5845766e-01,  3.3164865e-01, -2.8627914e-01,\n",
       "       -3.6947533e-02,  1.8208618e-01, -3.1218007e-01,  7.5630613e-02,\n",
       "       -1.4815192e-01,  1.3980363e-01, -6.3882720e-01,  3.7740152e-02,\n",
       "       -2.8488025e-01, -4.6945715e-01,  8.6255640e-02,  3.8550335e-01,\n",
       "        2.4608003e-01,  1.8022247e-01, -6.4713076e-02,  1.1324853e+00,\n",
       "       -3.5137454e-01, -2.6744670e-01, -2.8706011e-01, -2.1943483e-01,\n",
       "        2.8853205e-01,  2.3831853e-01,  6.5529160e-02,  3.3113310e-01,\n",
       "        7.0876971e-02, -1.5132450e-01,  2.1864030e-01,  1.0246916e-01,\n",
       "        2.2849274e-01, -2.8845105e-01,  1.4559811e-01,  2.4681638e-01,\n",
       "       -3.3567261e-02,  2.2350734e-01,  1.4540048e-01, -3.9889014e-01,\n",
       "        1.7679639e-01,  1.1940583e-01, -5.9695488e-01,  6.2303308e-02,\n",
       "        9.0350956e-02, -5.1355201e-01, -1.6860448e-01,  1.7907012e-01,\n",
       "       -1.3151786e-01, -5.3585225e-01,  5.5372530e-01,  1.5263624e-01,\n",
       "        1.9472370e-01, -4.2441699e-01, -1.6323459e-01, -4.7798982e-01,\n",
       "        4.4044580e-02,  2.6704711e-01, -1.5945318e-01,  4.3768996e-01,\n",
       "        5.6057807e-02, -1.6460758e-01,  5.5612870e-03, -2.1590443e-01,\n",
       "        3.2706583e-01,  4.6338856e-01,  1.2125514e-01, -2.0918895e-01,\n",
       "       -1.5291421e-01, -1.6878027e-01,  2.4842831e-01, -1.1893645e-01,\n",
       "       -9.8352142e-02, -1.5121040e-01, -4.9990144e-02,  2.6156977e-01,\n",
       "        3.9676604e-01,  8.7455265e-02,  4.2402539e-01,  1.3069777e-01,\n",
       "        1.0033098e-01, -3.0896699e-01,  2.7436331e-01,  5.0943017e-01,\n",
       "        2.2385600e-01,  1.8462241e-01, -1.4550216e-02, -6.6627815e-02,\n",
       "        3.2447495e-02,  7.9553239e-02,  8.1306472e-02,  5.2052486e-01,\n",
       "        1.7319682e-05, -3.0527040e-01,  8.3306812e-02, -4.2508128e-01,\n",
       "        2.5001556e-01, -2.8988263e-01, -5.1378781e-01, -6.3035064e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.get_vector(\"Putin\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "74aae071-b990-4179-b857-64b369834cf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Erdogan', 0.9224411249160767),\n",
       " ('Assad', 0.9186140894889832),\n",
       " ('Poroschenko', 0.9171161651611328),\n",
       " ('Hollande', 0.8956663012504578),\n",
       " ('Putins', 0.8930777907371521),\n",
       " ('Russlands', 0.8892519474029541),\n",
       " ('Syrien-Konflikt', 0.8809431195259094),\n",
       " ('Kreml', 0.8785954117774963),\n",
       " ('Obama', 0.8760547041893005),\n",
       " ('Lawrow', 0.8737961649894714)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(\"Putin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2257df8-4794-4346-a057-4b369dd658b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Königin', 0.7747876644134521),\n",
       " ('König', 0.7370375394821167),\n",
       " ('Elisabeth', 0.7069919109344482),\n",
       " ('Kaiserin', 0.6863002181053162),\n",
       " ('Ikone', 0.6776044368743896),\n",
       " ('Doris', 0.6739816069602966),\n",
       " ('Agnes', 0.6727083921432495),\n",
       " ('Maria', 0.6692684888839722),\n",
       " ('Katharina', 0.6676202416419983),\n",
       " ('Regisseurin', 0.665666401386261)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frau = w2v.get_vector(\"Frau\")\n",
    "mann = w2v.get_vector(\"Mann\")\n",
    "koenig = w2v.get_vector(\"König\")\n",
    "\n",
    "w2v.similar_by_vector(koenig - mann + frau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1487c91-3cc8-44dc-9a0f-540c911bf61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "paris = w2v.get_vector(\"Paris\")\n",
    "berlin = w2v.get_vector(\"Berlin\")\n",
    "deutschland = w2v.get_vector(\"Deutschland\")\n",
    "\n",
    "w2v.similar_by_vector(paris - berlin + deutschland)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37a50da-d351-4684-8f39-70c918c411e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "help(w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c8ea9-1aeb-4e34-af96-93f525261c85",
   "metadata": {},
   "source": [
    "## Aufgabe 1: Wortähnlichkeiten\n",
    "\n",
    "Der Vorteil von Word2Vec ist, dass die generierten Embeddings die Bedeutung von Worten widerspiegeln.\n",
    "Nutzen Sie die Methode `most_similar` des Modells, um ähnliche Worte für folgende Begriffe zu finden:\n",
    "\n",
    "- Stümper\n",
    "- Merkel\n",
    "- Hetze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623082f3-8798-480b-8c8b-17d54ce1a034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266b2af6-d887-4e18-aa53-d3836223cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def evaluate(classifier):\n",
    "    \"\"\"Evaluiere einen classifier auf den Testdaten\"\"\"\n",
    "    predicted = classifier.predict(test_ds['text'])\n",
    "    print(f\"Confusion matrix:\\n{metrics.confusion_matrix(test_ds['coarse'], predicted)}\")\n",
    "    print(f\"{metrics.classification_report(test_ds['coarse'], predicted)}\")\n",
    "    return np.mean(predicted == test_ds['coarse'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50171915-5ae9-4d78-89ba-f26ff67ea89b",
   "metadata": {},
   "source": [
    "## Aufgabe 2: Wortvektoren zur Textklassifikation\n",
    "\n",
    "Wir wollen nun untersuchen, inwiefern die Wortvektoren uns bei der Klassifikation der Tweets helfen. Die folgende Klasse \n",
    "`EmbeddingVectorizer` ordnet jedem Text den \"Durchnittsvektor\" seiner Worte zu. Dazu verwendet sie die Methode `get_mean_vector`.\n",
    "\n",
    "Verwenden Sie den `EmbeddingVectorizer` und einen `LinearSVC`, um die Tweets zu klassifizieren.\n",
    "\n",
    "Trainieren Sie den Klassifikator auf den Trainingsdaten und evaluieren Sie ihn auf den Testdaten. \n",
    "Wie gut ist Ihr Ergebnis? Vergleichen Sie Ihr Ergebnis mit dem Ergebnis vom letzten Mal und den [Ergebnissen des GermEval-2018](../data/GermEval-2018/results.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc9dff9-6e9f-488b-8001-027c5bfcdc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "class EmbeddingVectorizer(BaseEstimator):\n",
    "    \"\"\"Convert a collection of text documents to a matrix of vectors created from word embeddings \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.tokenizer = nltk.tokenize.casual.TweetTokenizer()\n",
    "    \n",
    "        \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        \"\"\"Nothing to do here, we use a pre-trained model. \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, raw_documents):\n",
    "        \"\"\"Transform documents to embedding matrix by mean of the embeddings\n",
    "        of individual words.\n",
    "        \"\"\"\n",
    "        if isinstance(raw_documents, str):\n",
    "            raise ValueError(\n",
    "                \"Iterable over raw text documents expected, \"\n",
    "                \"string object received.\")\n",
    "\n",
    "        _X = []\n",
    "        for doc in raw_documents:\n",
    "            words = self.tokenizer.tokenize(doc)\n",
    "            _X.append(self.model.get_mean_vector(words))\n",
    "            \n",
    "        X = np.array(_X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8906fe69-ef61-404b-b2a1-6209e6aa6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = EmbeddingVectorizer(w2v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
