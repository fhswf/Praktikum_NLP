{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassifikation von Texten mithilfe von Transformern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ja, ich kann Ihnen helfen. Um Twitter-Nachrichten mit einem Transformer-Modell zu klassifizieren, folgen Sie diesen Schritten:\n",
    "\n",
    "1. Installieren Sie die erforderlichen Bibliotheken:\n",
    "\n",
    "```bash\n",
    "pip install torch transformers tqdm scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Importieren Sie die erforderlichen Bibliotheken:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implementieren Sie eine benutzerdefinierte Dataset-Klasse:\n",
    "\n",
    "```python\n",
    "class GermEvalDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data_path, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = pd.read_csv(data_path, sep='\\t', names=['id', 'text', 'label'])\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.loc[index, 'text']\n",
    "        label = self.data.loc[index, 'label']\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "        \n",
    "        if label == \"OTHER\":\n",
    "            label_tensor = torch.tensor(0)\n",
    "        elif label == \"OFFENSE\":\n",
    "            label_tensor = torch.tensor(1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid label\")\n",
    "            \n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": label_tensor}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Laden Sie das pre-trained Modell und den Tokenizer:\n",
    "\n",
    "```python\n",
    "model_name = \"deepset/gbert-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Erstellen Sie die DataLoader f√ºr Training und Validierung:\n",
    "\n",
    "```python\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "train_data_path = \"path_to_train_data.tsv\"\n",
    "val_data_path = \"path_to_val_data.tsv\"\n",
    "\n",
    "train_dataset = GermEvalDataset(tokenizer, train_data_path, MAX_LEN)\n",
    "val_dataset = GermEvalDataset(tokenizer, val_data_path, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Erstellen Sie die Trainings-Argumente:\n",
    "\n",
    "```python\n",
    "training_args = Training\n",
    "\n",
    "Arguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    seed=42,\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Definieren Sie die Metrik-Funktion:\n",
    "\n",
    "```python\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    targets = p.label_ids\n",
    "    precision = precision_score(targets, preds)\n",
    "    recall = recall_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds)\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": acc}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Erstellen Sie einen Trainer und trainieren Sie das Modell:\n",
    "\n",
    "```python\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Optional: Bewerten Sie das Modell nach dem Training:\n",
    "\n",
    "```python\n",
    "trainer.evaluate()\n",
    "```\n",
    "\n",
    "Das trainierte Modell kann jetzt zur Klassifikation von Twitter-Nachrichten verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
