{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassifikation von Texten mithilfe von Transformern\n",
    "\n",
    "*Transformer* sind seit ChatGPT in aller Munde. Mit den \"kleinen Geschwistern\" von GPT kann man sehr gut Texte klassifizieren und andere NLP-Aufgaben lösen.\n",
    "Die folgende Beschreibung wie das funktioniert stammt übrigens zu großen Teilen von ChatGPT, lediglich an einigen Stellen habe ich etwas geändert (damit Sie auch noch selbst etwas zu tun haben)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ja, ich kann Ihnen helfen. Um Twitter-Nachrichten mit einem Transformer-Modell zu klassifizieren, folgen Sie diesen Schritten:\n",
    "\n",
    "1. Installieren Sie die erforderlichen Bibliotheken:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install torch transformers tqdm scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Importieren Sie die erforderlichen Bibliotheken:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implementieren Sie eine benutzerdefinierte Dataset-Klasse\n",
    "\n",
    "**Aufgabe 1: Ergänzen Sie Code zum Bereinigen der Tweets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GermEvalDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data_path, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = pd.read_csv(data_path, sep='\\t', header=None, names=['text', 'label', 'fine'])\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        #   Clean Tweets\n",
    "        ###\n",
    "        \n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.loc[index, 'text']\n",
    "        label = self.data.loc[index, 'label']\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "        \n",
    "        if label == \"OTHER\":\n",
    "            label_tensor = torch.tensor(0)\n",
    "        elif label == \"OFFENSE\":\n",
    "            label_tensor = torch.tensor(1)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid label: {label} for {text} at {index}\")\n",
    "            \n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": label_tensor}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Laden Sie das pre-trained Modell und den Tokenizer:\n",
    "\n",
    "**ChatGPT schlägt hier das Modell `\"deepset/gbert-large\"` vor – eine gute Wahl für deutschsprachige Tweets.\n",
    "Recherchieren Sie im [Model-Hub von Higging Face](https://huggingface.co/models) ein paar Alternativen und vergleichen Sie die Ergebnisse.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"deepset/gbert-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Erstellen Sie die DataLoader für Training und Validierung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 16\n",
    "train_data_path = \"../data/GermEval-2018/germeval2018.training.txt\"\n",
    "val_data_path = \"../data/GermEval-2018/germeval2018.test.txt\"\n",
    "\n",
    "train_dataset = GermEvalDataset(tokenizer, train_data_path, MAX_LEN)\n",
    "val_dataset = GermEvalDataset(tokenizer, val_data_path, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Erstellen Sie die Trainings-Argumente:\n",
    "\n",
    "**Die Vorgaben von ChatGPT sind in Ordnung, aber schauen Sie einmal, was passiert, wenn Sie an den Parametern `BATCH_SIZE` und `learning_rate` \"drehen\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Definieren Sie die Metrik-Funktion:\n",
    "\n",
    "**ChatGPT verwendet hier Metriken von *Scikit Learn*. Eine Alternative wären die Metriken aus `huggingface.co/metrics`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    targets = p.label_ids\n",
    "    precision = precision_score(targets, preds)\n",
    "    recall = recall_score(targets, preds)\n",
    "    f1 = f1_score(targets, preds)\n",
    "    acc = accuracy_score(targets, preds)\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1, \"accuracy\": acc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Erstellen Sie einen Trainer und trainieren Sie das Modell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Optional: Bewerten Sie das Modell nach dem Training:\n",
    "\n",
    "```python\n",
    "trainer.evaluate()\n",
    "```\n",
    "\n",
    "Das trainierte Modell kann jetzt zur Klassifikation von Twitter-Nachrichten verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
