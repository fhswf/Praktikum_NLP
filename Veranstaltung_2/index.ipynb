{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21dc3826-60ec-46e9-b914-0d4cfc14a7aa",
   "metadata": {},
   "source": [
    "# Textklassifikation, Teil 2\n",
    "\n",
    "Im ersten Teil haben wir Texte aus dem Datensatz *GermEval 2018* mithilfe von Frequency-Vektoren und einem Naive-Bayes-Klassifikator klassifiziert.\n",
    "\n",
    "Dieses Mal verwenden wir ein vortrainiertes *Word2Vec* Modell und wollen untersuchen, ob wir damit bessere Ergebnisse erzielen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6068a489-3553-4e3a-a262-644fbabe3308",
   "metadata": {},
   "source": [
    "## Daten einlesen\n",
    "\n",
    "Wie beim letzten Mal lesen wir zunächst die Daten ein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95a2f351-30b1-453f-8a1e-61bc5911a45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_ds = pd.read_csv('../data/GermEval-2018/germeval2018.training.txt', sep='\\t', header=None, names=['text', 'coarse', 'fine'])\n",
    "test_ds = pd.read_csv('../data/GermEval-2018/germeval2018.test.txt', sep='\\t', header=None, names=['text', 'coarse', 'fine'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1db8cf6b-7154-4ab3-b558-96fb5f957cfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>coarse</th>\n",
       "      <th>fine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@corinnamilborn Liebe Corinna, wir würden dich...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@Martin28a Sie haben ja auch Recht. Unser Twee...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@ahrens_theo fröhlicher gruß aus der schönsten...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@dushanwegner Amis hätten alles und jeden gewä...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@spdde kein verläßlicher Verhandlungspartner. ...</td>\n",
       "      <td>OFFENSE</td>\n",
       "      <td>INSULT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5004</th>\n",
       "      <td>Gegens. Zul. zu Patenamt &amp;amp; gegenseitige An...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5005</th>\n",
       "      <td>@GlasenappHenrik Zu Merkel fällt mir nur ein, ...</td>\n",
       "      <td>OFFENSE</td>\n",
       "      <td>INSULT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5006</th>\n",
       "      <td>@KokoLores20 @krippmarie Ein richtiges Zeichen...</td>\n",
       "      <td>OFFENSE</td>\n",
       "      <td>ABUSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5007</th>\n",
       "      <td>@Hartes_Geld ,Honecker‘Merkel macht uns zur ,D...</td>\n",
       "      <td>OFFENSE</td>\n",
       "      <td>ABUSE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5008</th>\n",
       "      <td>Warum wurden die G20-Chaoten nicht sofort auf ...</td>\n",
       "      <td>OTHER</td>\n",
       "      <td>OTHER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5009 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text   coarse    fine\n",
       "0     @corinnamilborn Liebe Corinna, wir würden dich...    OTHER   OTHER\n",
       "1     @Martin28a Sie haben ja auch Recht. Unser Twee...    OTHER   OTHER\n",
       "2     @ahrens_theo fröhlicher gruß aus der schönsten...    OTHER   OTHER\n",
       "3     @dushanwegner Amis hätten alles und jeden gewä...    OTHER   OTHER\n",
       "4     @spdde kein verläßlicher Verhandlungspartner. ...  OFFENSE  INSULT\n",
       "...                                                 ...      ...     ...\n",
       "5004  Gegens. Zul. zu Patenamt &amp; gegenseitige An...    OTHER   OTHER\n",
       "5005  @GlasenappHenrik Zu Merkel fällt mir nur ein, ...  OFFENSE  INSULT\n",
       "5006  @KokoLores20 @krippmarie Ein richtiges Zeichen...  OFFENSE   ABUSE\n",
       "5007  @Hartes_Geld ,Honecker‘Merkel macht uns zur ,D...  OFFENSE   ABUSE\n",
       "5008  Warum wurden die G20-Chaoten nicht sofort auf ...    OTHER   OTHER\n",
       "\n",
       "[5009 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd3b27d-4cec-4308-8943-1c1ebde8d802",
   "metadata": {},
   "source": [
    "## Texte bereinigen\n",
    "\n",
    "Auch die Bereinigung der Texte hatten wir schon beim letzten Mal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b6fb4753-5947-4983-9d07-6c5643d4b9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_tweet(text):\n",
    "    \"\"\" Preprocess and tokenize a tweet. \"\"\"\n",
    "    \n",
    "    # remove handles, i.e. @username\n",
    "    text = re.sub('\\@\\w+', '', text)\n",
    "\n",
    "    # remove hashtags, quotes, etc.\n",
    "    text = re.sub('[\\#\"\\']+', '', text)\n",
    "    \n",
    "    # replace hyphens with blanks\n",
    "    text = text.replace('-', ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a06ef57e-8191-402c-864b-d44ca111b739",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds['text'] = train_ds['text'].map(clean_tweet)\n",
    "test_ds['text'] = test_ds['text'].map(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e1802236-7b3d-4276-8f44-c8d836babdeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.10/site-packages (4.3.1)\n",
      "Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (3.8.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.10/site-packages (from gensim) (6.3.0)\n",
      "Requirement already satisfied: scipy>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.9.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /opt/conda/lib/python3.10/site-packages (from gensim) (1.22.4)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.10/site-packages (from nltk) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from nltk) (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca42c9c-bbba-4e92-9226-beac24a7239b",
   "metadata": {},
   "source": [
    "## Word2Vec-Modell laden\n",
    "\n",
    "Die Datei `twitter-de_d100_w5_min10.bin` enthält ein mit deuschen Twitternachrichten trainiertes *Word2Vec* Modell, das wir nun laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5810e049-4724-4abe-a5fb-38533b23d279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('/data/nlp/twitter-de_d100_w5_min10.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069c8ea9-1aeb-4e34-af96-93f525261c85",
   "metadata": {},
   "source": [
    "## Aufgabe 1: Wortähnlichkeiten\n",
    "\n",
    "Der Vorteil von Word2Vec ist, dass die generierten Embeddings die Bedeutung von Worten widerspiegeln.\n",
    "Nutzen Sie die Methode `most_similar` des Modells, um ähnliche Worte für folgende Begriffe zu finden:\n",
    "\n",
    "- Stümper\n",
    "- Merkel\n",
    "- Hetze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "266b2af6-d887-4e18-aa53-d3836223cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def evaluate(classifier):\n",
    "    \"\"\"Evaluiere einen classifier auf den Testdaten\"\"\"\n",
    "    predicted = classifier.predict(test_ds['text'])\n",
    "    print(f\"Confusion matrix:\\n{metrics.confusion_matrix(test_ds['coarse'], predicted)}\")\n",
    "    print(f\"{metrics.classification_report(test_ds['coarse'], predicted)}\")\n",
    "    return np.mean(predicted == test_ds['coarse'])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50171915-5ae9-4d78-89ba-f26ff67ea89b",
   "metadata": {},
   "source": [
    "## Aufgabe 2: Wortvektoren zur Textklassifikation\n",
    "\n",
    "Wir wollen nun untersuchen, inwiefern die Wortvektoren uns bei der Klassifikation der Tweets helfen. Die folgende Klasse \n",
    "`EmbeddingVectorizer` ordnet jedem Text den \"Durchnittsvektor\" seiner Worte zu. Dazu verwendet sie die Methode `get_mean_vector`.\n",
    "\n",
    "Verwenden Sie den `EmbeddingVectorizer` und einen `LinearSVC`, um die Tweets zu klassifizieren.\n",
    "\n",
    "Trainieren Sie den Klassifikator auf den Trainingsdaten und evaluieren Sie ihn auf den Testdaten. \n",
    "Wie gut ist Ihr Ergebnis? Vergleichen Sie Ihr Ergebnis mit dem Ergebnis vom letzten Mal und den [Ergebnissen des GermEval-2018](../data/GermEval-2018/results.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3dc9dff9-6e9f-488b-8001-027c5bfcdc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "class EmbeddingVectorizer(BaseEstimator):\n",
    "    \"\"\"Convert a collection of text documents to a matrix of vectors created from word embeddings \"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.tokenizer = nltk.tokenize.casual.TweetTokenizer()\n",
    "    \n",
    "        \n",
    "    def fit(self, X, y, **fit_params):\n",
    "        \"\"\"Nothing to do here, we use a pre-trained model. \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, raw_documents):\n",
    "        \"\"\"Transform documents to embedding matrix by mean of the embeddings\n",
    "        of individual words.\n",
    "        \"\"\"\n",
    "        if isinstance(raw_documents, str):\n",
    "            raise ValueError(\n",
    "                \"Iterable over raw text documents expected, \"\n",
    "                \"string object received.\")\n",
    "\n",
    "        _X = []\n",
    "        for doc in raw_documents:\n",
    "            words = self.tokenizer.tokenize(doc)\n",
    "            _X.append(self.model.get_mean_vector(words))\n",
    "            \n",
    "        X = np.array(_X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8906fe69-ef61-404b-b2a1-6209e6aa6ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = EmbeddingVectorizer(w2v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
