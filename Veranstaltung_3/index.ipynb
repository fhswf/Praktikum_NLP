{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Klassifikation von Texten mithilfe von Transformern\n",
    "\n",
    "*Transformer* sind seit ChatGPT in aller Munde. Mit den \"kleinen Geschwistern\" von GPT kann man sehr gut Texte klassifizieren und andere NLP-Aufgaben lösen.\n",
    "Die folgende Beschreibung wie das funktioniert stammt übrigens zu großen Teilen von ChatGPT, lediglich an einigen Stellen habe ich etwas geändert (damit Sie auch noch selbst etwas zu tun haben)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ja, ich kann Ihnen helfen. Um Twitter-Nachrichten mit einem Transformer-Modell zu klassifizieren, folgen Sie diesen Schritten:\n",
    "\n",
    "1. Installieren Sie die erforderlichen Bibliotheken:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers tqdm scikit-learn --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Importieren Sie die erforderlichen Bibliotheken:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implementieren Sie eine benutzerdefinierte Dataset-Klasse\n",
    "\n",
    "**Aufgabe 1: Ergänzen Sie Code zum Bereinigen der Tweets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GermEvalDataset(Dataset):\n",
    "    def __init__(self, tokenizer, data_path, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = pd.read_csv(data_path, sep='\\t', header=None, names=['text', 'label', 'fine'])\n",
    "        \n",
    "        ### YOUR CODE HERE\n",
    "        #   Clean Tweets\n",
    "        ###\n",
    "        \n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = self.data.loc[index, 'text']\n",
    "        label = self.data.loc[index, 'label']\n",
    "        \n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids'][0]\n",
    "        attention_mask = inputs['attention_mask'][0]\n",
    "        \n",
    "        if label == \"OTHER\":\n",
    "            label_tensor = torch.tensor(0)\n",
    "        elif label == \"OFFENSE\":\n",
    "            label_tensor = torch.tensor(1)\n",
    "        else:\n",
    "            raise ValueError(f\"Invalid label: {label} for {text} at {index}\")\n",
    "            \n",
    "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": label_tensor}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Laden Sie das pre-trained Modell und den Tokenizer:\n",
    "\n",
    "**ChatGPT schlägt hier das Modell `\"deepset/gbert-large\"` vor – eine gute Wahl für deutschsprachige Tweets.\n",
    "Recherchieren Sie im [Model-Hub von Higging Face](https://huggingface.co/models) ein paar Alternativen und vergleichen Sie die Ergebnisse.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e31fdcd1a8841e69a7ba25d4d60628b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/83.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33e4d588dfc94dcfa1be00db921ea074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1455dfce9154b94880e6b688d462045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/240k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de99ec8bc0a0479888bf3caa33367549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.35G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at deepset/gbert-large and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"deepset/gbert-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Erstellen Sie die DataLoader für Training und Validierung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 16\n",
    "train_data_path = \"../data/GermEval-2018/germeval2018.training.txt\"\n",
    "val_data_path = \"../data/GermEval-2018/germeval2018.test.txt\"\n",
    "\n",
    "train_dataset = GermEvalDataset(tokenizer, train_data_path, MAX_LEN)\n",
    "val_dataset = GermEvalDataset(tokenizer, val_data_path, MAX_LEN)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Erstellen Sie die Trainings-Argumente:\n",
    "\n",
    "**Die Vorgaben von ChatGPT sind in Ordnung, aber schauen Sie einmal, was passiert, wenn Sie an den Parametern `BATCH_SIZE` und `learning_rate` \"drehen\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    report_to=None,\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    seed=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Definieren Sie die Metrik-Funktion:\n",
    "\n",
    "**Hier habe ich geschummelt und die Metriken aus GermEval 2018 \"nachgebaut\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = np.argmax(preds, axis=1)\n",
    "    accuracy = (preds == p.label_ids).astype(np.float32).mean().item()\n",
    "    metrics = { \"accuracy\": accuracy }\n",
    "    for val, key in enumerate(['OTHER', 'OFFENSE']):\n",
    "        tp = ((preds == p.label_ids) * (preds == val)).sum().item()\n",
    "        fp = ((preds != p.label_ids) * (preds == val)).sum().item()\n",
    "        fn = ((preds != p.label_ids) * (preds != val)).sum().item()\n",
    "\n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        metrics[f\"precision_{key}\"] = precision\n",
    "        metrics[f\"recall_{key}\"] = recall\n",
    "        metrics[f\"f1_{key}\"] = f1\n",
    "        \n",
    "    metrics[f\"precision_AVERAGE\"] = 0.5 * (metrics[f\"precision_OTHER\"] + metrics[f\"precision_OFFENSE\"])\n",
    "    metrics[f\"recall_AVERAGE\"] = 0.5 * (metrics[f\"recall_OTHER\"] + metrics[f\"recall_OFFENSE\"])\n",
    "    metrics[f\"f1_AVERAGE\"] = 0.5 * (metrics[f\"f1_OTHER\"] + metrics[f\"f1_OFFENSE\"])\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Erstellen Sie einen Trainer und trainieren Sie das Modell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1570' max='1570' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1570/1570 47:37, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision Other</th>\n",
       "      <th>Recall Other</th>\n",
       "      <th>F1 Other</th>\n",
       "      <th>Precision Offense</th>\n",
       "      <th>Recall Offense</th>\n",
       "      <th>F1 Offense</th>\n",
       "      <th>Precision Average</th>\n",
       "      <th>Recall Average</th>\n",
       "      <th>F1 Average</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.487600</td>\n",
       "      <td>0.478014</td>\n",
       "      <td>0.815480</td>\n",
       "      <td>0.811611</td>\n",
       "      <td>0.939057</td>\n",
       "      <td>0.870695</td>\n",
       "      <td>0.828105</td>\n",
       "      <td>0.573913</td>\n",
       "      <td>0.677966</td>\n",
       "      <td>0.819858</td>\n",
       "      <td>0.756485</td>\n",
       "      <td>0.774331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.400600</td>\n",
       "      <td>0.477481</td>\n",
       "      <td>0.811948</td>\n",
       "      <td>0.797192</td>\n",
       "      <td>0.959964</td>\n",
       "      <td>0.871039</td>\n",
       "      <td>0.869754</td>\n",
       "      <td>0.522609</td>\n",
       "      <td>0.652906</td>\n",
       "      <td>0.833473</td>\n",
       "      <td>0.741287</td>\n",
       "      <td>0.761973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.355800</td>\n",
       "      <td>0.420322</td>\n",
       "      <td>0.829606</td>\n",
       "      <td>0.829972</td>\n",
       "      <td>0.933719</td>\n",
       "      <td>0.878794</td>\n",
       "      <td>0.828539</td>\n",
       "      <td>0.626087</td>\n",
       "      <td>0.713224</td>\n",
       "      <td>0.829255</td>\n",
       "      <td>0.779903</td>\n",
       "      <td>0.796009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.244800</td>\n",
       "      <td>0.562521</td>\n",
       "      <td>0.829606</td>\n",
       "      <td>0.819365</td>\n",
       "      <td>0.952402</td>\n",
       "      <td>0.880889</td>\n",
       "      <td>0.863694</td>\n",
       "      <td>0.589565</td>\n",
       "      <td>0.700775</td>\n",
       "      <td>0.841529</td>\n",
       "      <td>0.770984</td>\n",
       "      <td>0.790832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.278800</td>\n",
       "      <td>0.444283</td>\n",
       "      <td>0.834903</td>\n",
       "      <td>0.859395</td>\n",
       "      <td>0.897242</td>\n",
       "      <td>0.877911</td>\n",
       "      <td>0.780209</td>\n",
       "      <td>0.713043</td>\n",
       "      <td>0.745116</td>\n",
       "      <td>0.819802</td>\n",
       "      <td>0.805143</td>\n",
       "      <td>0.811513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.212300</td>\n",
       "      <td>0.536638</td>\n",
       "      <td>0.829311</td>\n",
       "      <td>0.830166</td>\n",
       "      <td>0.932829</td>\n",
       "      <td>0.878509</td>\n",
       "      <td>0.826835</td>\n",
       "      <td>0.626957</td>\n",
       "      <td>0.713155</td>\n",
       "      <td>0.828501</td>\n",
       "      <td>0.779893</td>\n",
       "      <td>0.795832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.136500</td>\n",
       "      <td>0.697891</td>\n",
       "      <td>0.837551</td>\n",
       "      <td>0.858714</td>\n",
       "      <td>0.903025</td>\n",
       "      <td>0.880312</td>\n",
       "      <td>0.789168</td>\n",
       "      <td>0.709565</td>\n",
       "      <td>0.747253</td>\n",
       "      <td>0.823941</td>\n",
       "      <td>0.806295</td>\n",
       "      <td>0.813782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.108300</td>\n",
       "      <td>0.819900</td>\n",
       "      <td>0.839612</td>\n",
       "      <td>0.852296</td>\n",
       "      <td>0.916370</td>\n",
       "      <td>0.883173</td>\n",
       "      <td>0.808359</td>\n",
       "      <td>0.689565</td>\n",
       "      <td>0.744252</td>\n",
       "      <td>0.830328</td>\n",
       "      <td>0.802968</td>\n",
       "      <td>0.813712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.154500</td>\n",
       "      <td>0.861723</td>\n",
       "      <td>0.819305</td>\n",
       "      <td>0.806912</td>\n",
       "      <td>0.955516</td>\n",
       "      <td>0.874949</td>\n",
       "      <td>0.864130</td>\n",
       "      <td>0.553043</td>\n",
       "      <td>0.674443</td>\n",
       "      <td>0.835521</td>\n",
       "      <td>0.754280</td>\n",
       "      <td>0.774696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.084100</td>\n",
       "      <td>1.071701</td>\n",
       "      <td>0.821660</td>\n",
       "      <td>0.809578</td>\n",
       "      <td>0.955071</td>\n",
       "      <td>0.876327</td>\n",
       "      <td>0.864611</td>\n",
       "      <td>0.560870</td>\n",
       "      <td>0.680380</td>\n",
       "      <td>0.837094</td>\n",
       "      <td>0.757970</td>\n",
       "      <td>0.778353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.060500</td>\n",
       "      <td>0.997131</td>\n",
       "      <td>0.834020</td>\n",
       "      <td>0.843113</td>\n",
       "      <td>0.920374</td>\n",
       "      <td>0.880051</td>\n",
       "      <td>0.810381</td>\n",
       "      <td>0.665217</td>\n",
       "      <td>0.730659</td>\n",
       "      <td>0.826747</td>\n",
       "      <td>0.792796</td>\n",
       "      <td>0.805355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.065600</td>\n",
       "      <td>0.990534</td>\n",
       "      <td>0.831960</td>\n",
       "      <td>0.844920</td>\n",
       "      <td>0.913701</td>\n",
       "      <td>0.877965</td>\n",
       "      <td>0.799380</td>\n",
       "      <td>0.672174</td>\n",
       "      <td>0.730279</td>\n",
       "      <td>0.822150</td>\n",
       "      <td>0.792937</td>\n",
       "      <td>0.804122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>1.047110</td>\n",
       "      <td>0.834903</td>\n",
       "      <td>0.846977</td>\n",
       "      <td>0.915925</td>\n",
       "      <td>0.880103</td>\n",
       "      <td>0.804550</td>\n",
       "      <td>0.676522</td>\n",
       "      <td>0.735002</td>\n",
       "      <td>0.825763</td>\n",
       "      <td>0.796224</td>\n",
       "      <td>0.807552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>1.111296</td>\n",
       "      <td>0.832843</td>\n",
       "      <td>0.839806</td>\n",
       "      <td>0.923488</td>\n",
       "      <td>0.879661</td>\n",
       "      <td>0.814255</td>\n",
       "      <td>0.655652</td>\n",
       "      <td>0.726397</td>\n",
       "      <td>0.827030</td>\n",
       "      <td>0.789570</td>\n",
       "      <td>0.803029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>1.128600</td>\n",
       "      <td>0.833726</td>\n",
       "      <td>0.841657</td>\n",
       "      <td>0.922153</td>\n",
       "      <td>0.880068</td>\n",
       "      <td>0.812834</td>\n",
       "      <td>0.660870</td>\n",
       "      <td>0.729017</td>\n",
       "      <td>0.827245</td>\n",
       "      <td>0.791511</td>\n",
       "      <td>0.804542</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1570, training_loss=0.17342701322713475, metrics={'train_runtime': 2858.4847, 'train_samples_per_second': 8.762, 'train_steps_per_second': 0.549, 'total_flos': 2.334022099454976e+16, 'train_loss': 0.17342701322713475, 'epoch': 5.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: wandb [OPTIONS] COMMAND [ARGS]...\n",
      "Try 'wandb --help' for help.\n",
      "\n",
      "Error: No such command 'logout'.\n"
     ]
    }
   ],
   "source": [
    "!wandb logout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Optional: Bewerten Sie das Modell nach dem Training:\n",
    "\n",
    "```python\n",
    "trainer.evaluate()\n",
    "```\n",
    "\n",
    "Das trainierte Modell kann jetzt zur Klassifikation von Twitter-Nachrichten verwendet werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
